From 4b7aff67b32dffbbad3fbc96f2fc72dc33053b75 Mon Sep 17 00:00:00 2001
From: Arnaud Rebillout <arnaud.rebillout@collabora.com>
Date: Sat, 23 Mar 2019 16:54:23 +0700
Subject: [PATCH 2/2] Rewrite casync-http to use curl multi

This commit brings parallel downloads to casync-http, by using the curl
multi interface, and more precisely the "select()" flavour. For details
see <https://curl.haxx.se/libcurl/c/libcurl-multi.html>.

Libcurl has two ways to achieve parallel downloads:
- for HTTP/1, it can open parallel connections. The maximum number of
  parallel connections is user-defined, through `MAX_HOST_CONNECTIONS`
  and `MAX_TOTAL_CONNECTIONS`.
- for HTTP/2, it can attempt to multiplex on a single connection. The
  maximum number of parallel downloads in this case is negociated
  between the client and the server (we talk about number of streams in
  the HTTP/2 jargon).
- (note that libcurl used to do pipelining over HTTP/1.1, but this is no
  longer supported since 7.62.0, and casync-http doesn't use it anyway)

Accordingly, this commit also introduces two new command-line arguments
to better control the behavior of parallel downloads:
- `--max-active-chunks` is the sum of 1. the number of chunks in the
  curl multi and 2. chunks downloaded and waiting to be sent to the
  remote.  It allows to limit the number of chunks waiting in RAM, in
  case we download faster than we can send to remote. It also gives a
  limit for the maximum number of concurrent downloads.
- `--max-host-connections` is for the case where libcurl opens parallel
  connections to the server. In all likelihood it's only used for HTTP1.

We probably want a large number for max-active-chunks, to ensure we
don't starve the libcurl multi handle, but at the same time we probably
don't want to open too many connections in parallel, and that's why
max-host-connections is a much lower number. It seems to be a sensible
default, according to my understanding so far. User might want to adjust
these number for their specific use-case.

Note that the command-line argument `--rate-limit-bps` doesn't make much
sense anymore, since it's set for each chunk, but now chunks are
downloaded in parallel, and we don't really know how many downloads are
actually happening in parallel. And from Daniel Steinberg:

    We don't have settings that limit the transfer speed of multiple,
    combined, transfers.

So we might want to completely remove this option, or rework it somehow.

Note that this commit removes the wrapper `robust_curl_easy_perform()`
introduced in 328f13d. Quick reminder: this wrapper was used to sleep
and retry on `CURLE_COULDNT_CONNECT`, and allowed to workaround what
seemed to be a misbehavior of the Python Simple HTTP Server. Now that we
do parallel downloads, we can't apply this workaround "as is", we can't
just sleep. So I removed the wrapper. The issue is still present and
reproducible though, but I just assume it's a server issue, not a casync
issue.

Regarding unit tests
--------------------

This commit also opens interesting questions regarding the unit tests.
For now we're using the Python Simple HTTP Server, which can only serve
requests sequentially. It doesn't allow to really test parallelism. It's
not really representative of real-life scenario where, I assume, chunks
are served by a production server such as apache or nginx. Additionally,
I think it would be best to run the test for both a HTTP/1 server and a
HTTP/2 server.

One possibility is to use nginx, it's easy enough to run it. nginx can
serve HTTP/2 only if TLS is enabled though, and out of the box
casync-http will fail if it can't recognize the certificate. So we might
want to add a command-line argument to trust any random certificate.

Additionally, nginx requires root, maybe not very suitable for a test
suite, however there might be some workarounds?

Another possibility is to run nghttpd. This option is light on
dependencies, in the sense that libcurl already relies on libnhgttp2,
however I didn't succeed in using this server yet.

TODOs
-----

There are a few todos left, mainly:
- add argument or env variables support in casync-tool, to match new
  arguments in casync-http.
- documentation, readme and such.
- some details in the code, see MR in GitHub.

Signed-off-by: Arnaud Rebillout <arnaud.rebillout@collabora.com>
---
 meson.build       |    2 +-
 src/casync-http.c | 1422 +++++++++++++++++++++++++++++++++++----------
 2 files changed, 1107 insertions(+), 317 deletions(-)

diff --git a/meson.build b/meson.build
index aac7727..f226ef8 100644
--- a/meson.build
+++ b/meson.build
@@ -129,7 +129,7 @@ libzstd = dependency(
 conf.set10('HAVE_LIBZSTD', libzstd.found())
 
 libcurl = dependency('libcurl',
-                     version : '>= 7.29.0')
+                     version : '>= 7.43.0')
 openssl = dependency('openssl',
                      version : '>= 1.0')
 libacl = cc.find_library('acl')
diff --git a/src/casync-http.c b/src/casync-http.c
index 43809c3..e92f987 100644
--- a/src/casync-http.c
+++ b/src/casync-http.c
@@ -2,6 +2,7 @@
 
 #include <curl/curl.h>
 #include <getopt.h>
+#include <poll.h>
 #include <stddef.h>
 #include <unistd.h>
 
@@ -11,67 +12,1037 @@
 #include "realloc-buffer.h"
 #include "util.h"
 
+/* The maximum number of active chunks is defined as the sum of:
+ * - number of chunks added to curl multi for download
+ * - number of chunks downloaded, and waiting to be sent to remote
+ *
+ * In situations where the server is local and super fast (ie. we receive chunks
+ * faster than we can send them to the remote), around 95% of the active chunks
+ * are chunks waiting to be sent to remote, hence this number can be seen as
+ * "maximum number of chunks sitting in ram".
+ *
+ * In situations where the server is away, around 95% of the active chunks are
+ * chunks added to curl multi. It doesn't mean "being downloaded" though, it's more
+ * a "maximum limit for concurrent downloads". The real number of running downloads
+ * might be lower, because:
+ * - if we're doing HTTP/1 and parallel connections, the hard limit is actually
+ *   defined by `MAX_HOST_CONNECTIONS`.
+ * - if we're doing HTTP/2 over a multiplexed connection, the number of parallel
+ *   streams is negociated between client and server.
+ *
+ * In effect, *I think* it's best to make this number quite large, because we
+ * don't want to underfeed libcurl and underperform. I think it's better to feed
+ * too many handles to the curl multi, and let libcurl decide internally what's
+ * best to do with it. Libcurl knows every details about the HTTP connection and
+ * will handle (parallel/multiplex/whatever) downloads better than us.
+ */
+#define MAX_ACTIVE_CHUNKS 64
+
+/* This is the maximum number of parallel connections per host. This should have
+ * no effect in case we're doing HTTP/2 with one connection and multiplexing.
+ * However, if we're doing HTTP/1, curl will open parallel connections, as HTTP/1
+ * pipelining is no longer supported since libcurl 7.62.
+ *
+ * We want to make sure that we don't open too many parallel connections per host.
+ * It seems that the norm for web browsers ranges from 6 to 8.
+ */
+#define MAX_HOST_CONNECTIONS 8
+
+/* This is to enable excessively verbose logging. */
+#define ENABLE_LOG_TRACE 0
+
 static volatile sig_atomic_t quit = false;
 
-static bool arg_verbose = false;
-static curl_off_t arg_rate_limit_bps = 0;
+static bool arg_verbose = false;
+static curl_off_t arg_rate_limit_bps = 0;
+static uint64_t arg_max_active_chunks = MAX_ACTIVE_CHUNKS;
+static uint64_t arg_max_host_connections = MAX_HOST_CONNECTIONS;
+
+typedef enum Protocol {
+        PROTOCOL_HTTP,
+        PROTOCOL_FTP,
+        PROTOCOL_HTTPS,
+        PROTOCOL_SFTP,
+        _PROTOCOL_INVALID = -1,
+} Protocol;
+
+static Protocol arg_protocol = _PROTOCOL_INVALID;
+
+typedef enum ProcessUntil {
+        PROCESS_UNTIL_WRITTEN,
+        PROCESS_UNTIL_CAN_PUT_CHUNK,
+        PROCESS_UNTIL_CAN_PUT_INDEX,
+        PROCESS_UNTIL_CAN_PUT_ARCHIVE,
+        PROCESS_UNTIL_HAVE_REQUEST,
+        PROCESS_UNTIL_FINISHED,
+} ProcessUntil;
+
+/*
+ * log helpers
+ */
+
+#define log_trace(fmt, ...)                                             \
+        do {                                                            \
+                if (ENABLE_LOG_TRACE)                                   \
+                        log_debug("[%d] " fmt, (int) getpid(), ##__VA_ARGS__); \
+        } while (false)
+
+#define log_error_curle(code, fmt, ...) \
+        log_error(fmt ": %s", ##__VA_ARGS__, curl_easy_strerror(code))
+
+#define log_error_curlm(code, fmt, ...) \
+        log_error(fmt ": %s", ##__VA_ARGS__, curl_multi_strerror(code))
+
+/*
+ * protocol helpers
+ */
+
+static const char *protocol_str(Protocol protocol) {
+        switch (protocol) {
+        case PROTOCOL_HTTP:
+                return "HTTP";
+        case PROTOCOL_FTP:
+                return "FTP";
+        case PROTOCOL_HTTPS:
+                return "HTTPS";
+        case PROTOCOL_SFTP:
+                return "SFTP";
+        default:
+                assert_not_reached("Unknown protocol");
+        }
+}
+
+static bool protocol_status_ok(Protocol protocol, long protocol_status) {
+        switch (protocol) {
+        case PROTOCOL_HTTP:
+        case PROTOCOL_HTTPS:
+                if (protocol_status == 200)
+                        return true;
+                break;
+        case PROTOCOL_FTP:
+                if (protocol_status >= 200 && protocol_status <= 299)
+                        return true;
+                break;
+        case PROTOCOL_SFTP:
+                if (protocol_status == 0)
+                        return true;
+                break;
+        default:
+                assert_not_reached("Unknown protocol");
+                break;
+        }
+        return false;
+}
+
+/*
+ * curl helpers
+ */
+
+DEFINE_TRIVIAL_CLEANUP_FUNC(CURL*, curl_easy_cleanup);
+DEFINE_TRIVIAL_CLEANUP_FUNC(CURLM*, curl_multi_cleanup);
+
+static inline const char *get_curl_effective_url(CURL *handle) {
+        CURLcode c;
+        char *effective_url;
+
+        c = curl_easy_getinfo(handle, CURLINFO_EFFECTIVE_URL, &effective_url);
+        if (c != CURLE_OK) {
+                log_error_curle(c, "Failed to get CURL effective url");
+                return NULL;
+        }
+
+        return effective_url;
+}
+
+static inline void *get_curl_private(CURL *handle) {
+        CURLcode c;
+        void *private;
+
+        c = curl_easy_getinfo(handle, CURLINFO_PRIVATE, &private);
+        if (c != CURLE_OK) {
+                log_error_curle(c, "Failed to get CURL private data");
+                return NULL;
+        }
+
+        return private;
+}
+
+static int configure_curl_easy_handle(CURL *handle, const char *url) {
+        CURLcode c;
+
+        assert(handle);
+        assert(url);
+
+        c = curl_easy_setopt(handle, CURLOPT_URL, url);
+        if (c != CURLE_OK) {
+                log_error_curle(c, "Failed to set CURL URL to %s", url);
+                return -EIO;
+        }
+
+        return 0;
+}
+
+typedef size_t (*ca_curl_write_callback_t)(const void *, size_t, size_t, void *);
+
+static int make_curl_easy_handle(CURL **ret,
+                                 ca_curl_write_callback_t write_callback,
+                                 void *write_data, void *private) {
+        CURLcode c;
+        _cleanup_(curl_easy_cleanupp) CURL *handle = NULL;
+
+        assert(ret);
+        assert(write_callback);
+        assert(write_data);
+        /* private is optional and can be null */
+
+        handle = curl_easy_init();
+        if (!handle)
+                return log_oom();
+
+        c = curl_easy_setopt(handle, CURLOPT_FOLLOWLOCATION, 1L);
+        if (c != CURLE_OK) {
+                log_error_curle(c, "Failed to turn on CURL location following");
+                return -EIO;
+        }
+
+        c = curl_easy_setopt(handle, CURLOPT_PROTOCOLS,
+                             arg_protocol == PROTOCOL_FTP ? CURLPROTO_FTP :
+                             arg_protocol == PROTOCOL_SFTP? CURLPROTO_SFTP:
+                             CURLPROTO_HTTP | CURLPROTO_HTTPS);
+        if (c != CURLE_OK) {
+                log_error_curle(c, "Failed to limit CURL protocols to HTTP/HTTPS/FTP/SFTP");
+                return -EIO;
+        }
+
+        if (IN_SET(arg_protocol, PROTOCOL_HTTP, PROTOCOL_HTTPS)) {
+                /* Default since libcurl 7.62.0 */
+                c = curl_easy_setopt(handle, CURLOPT_HTTP_VERSION, CURL_HTTP_VERSION_2_0);
+                if (c != CURLE_OK)
+                        log_error_curle(c, "Failed to set CURL HTTP version to HTTP2, ignoring");
+
+                c = curl_easy_setopt(handle, CURLOPT_PIPEWAIT, 1L);
+                if (c != CURLE_OK)
+                        log_error_curle(c, "Failed to set CURL pipewait, ignoring");
+        }
+
+        if (arg_protocol == PROTOCOL_SFTP) {
+                /* activate the ssh agent. For this to work you need
+                   to have ssh-agent running (type set | grep SSH_AGENT to check) */
+                c = curl_easy_setopt(handle, CURLOPT_SSH_AUTH_TYPES, CURLSSH_AUTH_AGENT);
+                if (c != CURLE_OK)
+                        log_error_curle(c, "Failed to turn on CURL SSH agent support, ignoring");
+        }
+
+        if (arg_rate_limit_bps > 0) {
+                c = curl_easy_setopt(handle, CURLOPT_MAX_SEND_SPEED_LARGE, arg_rate_limit_bps);
+                if (c != CURLE_OK) {
+                        log_error_curle(c, "Failed to set CURL send speed limit");
+                        return -EIO;
+                }
+
+                c = curl_easy_setopt(handle, CURLOPT_MAX_RECV_SPEED_LARGE, arg_rate_limit_bps);
+                if (c != CURLE_OK) {
+                        log_error_curle(c, "Failed to set CURL receive speed limit");
+                        return -EIO;
+                }
+        }
+
+        c = curl_easy_setopt(handle, CURLOPT_WRITEFUNCTION, write_callback);
+        if (c != CURLE_OK) {
+                log_error_curle(c, "Failed to set CURL callback function");
+                return -EIO;
+        }
+
+        c = curl_easy_setopt(handle, CURLOPT_WRITEDATA, write_data);
+        if (c != CURLE_OK) {
+                log_error_curle(c, "Failed to set CURL callback data");
+                return -EIO;
+        }
+
+        if (private) {
+                c = curl_easy_setopt(handle, CURLOPT_PRIVATE, private);
+                if (c != CURLE_OK) {
+                        log_error_curle(c, "Failed to set CURL private data");
+                        return -EIO;
+                }
+        }
+
+        /* (void) curl_easy_setopt(handle, CURLOPT_SSL_VERIFYPEER, false); */
+
+        /* (void) curl_easy_setopt(handle, CURLOPT_VERBOSE, 1L); */
+
+        *ret = TAKE_PTR(handle);
+        return 0;
+}
+
+static int make_curl_multi_handle(CURLM **ret) {
+        CURLMcode c;
+        _cleanup_(curl_multi_cleanupp) CURLM *handle = NULL;
+
+        assert(ret);
+
+        handle = curl_multi_init();
+        if (!handle)
+                return log_oom();
+
+        c = curl_multi_setopt(handle, CURLMOPT_MAX_HOST_CONNECTIONS, arg_max_host_connections);
+        if (c < 0) {
+                log_error_curlm(c, "Failed to set CURL max host connections");
+                return -EIO;
+        }
+
+        /* Default since libcurl 7.62.0 */
+        c = curl_multi_setopt(handle, CURLMOPT_PIPELINING, CURLPIPE_MULTIPLEX);
+        if (c < 0)
+                log_error_curlm(c, "Failed to set CURL pipelining to multiplex, ignoring");
+
+        *ret = TAKE_PTR(handle);
+        return 0;
+}
+
+/*
+ * chunks data
+ */
+
+typedef struct ChunkData ChunkData;
+
+struct ChunkData {
+        size_t current_store;  /* set to SIZE_MAX if chunk is missing */
+        CaChunkID id;
+        ReallocBuffer buffer;
+};
+
+static void chunk_data_reset(ChunkData *cd, CaChunkID *id) {
+        assert(cd);
+
+        cd->id = *id;
+        realloc_buffer_empty(&cd->buffer);
+}
+
+static void chunk_data_free(ChunkData *cd) {
+        if (cd == NULL)
+                return;
+
+        realloc_buffer_free(&cd->buffer);
+        free(cd);
+}
+
+static ChunkData *chunk_data_new(void) {
+        ChunkData *cd = NULL;
+
+        cd = new0(ChunkData, 1);
+
+        return cd;
+}
+
+/*
+ * simple queue implementation
+ */
+
+typedef struct QueueItem QueueItem;
+
+struct QueueItem {
+        void *data;
+        QueueItem *next;
+};
+
+typedef struct Queue {
+        QueueItem *head;
+        QueueItem *tail;
+        uint64_t len;
+        uint64_t n_added;   /* total number of items added */
+        uint64_t n_removed; /* total number of items removed */
+} Queue;
+
+static int queue_push(Queue *q, void *data) {
+        int r;
+        QueueItem *item;
+
+        assert(q);
+        assert(data);
+
+        item = new0(QueueItem, 1);
+        if (!item) {
+                r = log_oom();
+                return r;
+        }
+
+        item->data = data;
+        item->next = NULL;
+
+        if (q->tail) {
+                q->tail->next = item;
+                q->tail = item;
+        } else {
+                assert(!q->head);
+                q->head = item;
+                q->tail = item;
+        }
+
+        q->n_added++;
+        q->len++;
+
+        return 0;
+}
+
+static void *queue_pop(Queue *q) {
+        QueueItem *item;
+        void *data;
+
+        assert(q);
+
+        item = q->head;
+        if (!item)
+                return NULL;
+
+        q->head = item->next;
+        if (!q->head)
+                q->tail = NULL;
+
+        data = item->data;
+        free(item);
+
+        q->n_removed++;
+        q->len--;
+
+        return data;
+}
+
+static void *queue_remove(Queue *q, void *data) {
+        QueueItem *curr, *prev;
+
+        assert(q);
+
+        for (prev = NULL, curr = q->head; curr; prev = curr, curr = curr->next)
+                if (curr->data == data)
+                        break;
+
+        if (curr == NULL)
+                return NULL; /* not found */
+
+        if (prev == NULL)
+                q->head = curr->next; /* removing first item */
+        else
+                prev->next = curr->next; /* removing later item */
+
+        if (q->tail == curr)
+                q->tail = prev; /* removed last item */
+
+        free(curr);
+
+        q->n_removed++;
+        q->len--;
+
+        return data;
+}
+
+static uint64_t queue_len(Queue *q) {
+        assert(q);
+
+        return q->len;
+}
+
+static bool queue_is_empty(Queue *q) {
+        assert(q);
+
+        return q->head == NULL;
+}
+
+static void queue_free(Queue *q) {
+        if (q == NULL)
+                return;
+
+        free(q);
+}
+
+static Queue *queue_new(void) {
+        return new0(Queue, 1);
+}
+
+/*
+ * Chunk Downloader
+ *
+ * We re-use things as much as possible, which means that:
+ * - CURL handles are allocated once at the beginning, then re-used all along.
+ * - ChunkData objects (ie. ReallocBuffer) as well.
+ *
+ * During operations, our CURL handles move from one queue to another, ie:
+ *   ready -> inprogress -> completed -> ready ...
+ *
+ * TODO This queues could be static (ie. of fixed size) to avoid a bunch of malloc/free.
+ */
+
+typedef struct CaChunkDownloader CaChunkDownloader;
+
+struct CaChunkDownloader {
+        CaRemote *remote;
+        CURLM *multi;
+        Queue *ready;       /* CURL handles waiting to be used */
+        Queue *inprogress;  /* CURL handles in use (ie. added to curl multi) */
+        Queue *completed;   /* CURL handles completed (ie. chunks waiting to be put to remote */
+
+        char *store_url;
+
+        uint64_t n_iterations;
+        uint64_t sum_inprogress_len;
+        uint64_t sum_completed_len;
+};
+
+enum {
+      CA_CHUNK_DOWNLOADER_FINISHED,
+      CA_CHUNK_DOWNLOADER_POLL
+};
+
+static char *chunk_url(const char *store_url, const CaChunkID *id) {
+        char ids[CA_CHUNK_ID_FORMAT_MAX], *buffer;
+        const char *suffix;
+        size_t n;
+
+        /* Chop off URL arguments and multiple trailing dashes, then append the chunk ID and ".cacnk" */
+
+        suffix = ca_compressed_chunk_suffix();
+
+        n = strcspn(store_url, "?;");
+        while (n > 0 && store_url[n-1] == '/')
+                n--;
+
+        buffer = new(char, n + 1 + 4 + 1 + CA_CHUNK_ID_FORMAT_MAX-1 + strlen(suffix) + 1);
+        if (!buffer)
+                return NULL;
+
+        ca_chunk_id_format(id, ids);
+
+        strcpy(mempcpy(mempcpy(mempcpy(mempcpy(mempcpy(buffer, store_url, n), "/", 1), ids, 4), "/", 1), ids, CA_CHUNK_ID_FORMAT_MAX-1), suffix);
+
+        return buffer;
+}
+
+static size_t write_chunk(const void *buffer, size_t size, size_t nmemb, void *userdata) {
+        ReallocBuffer *chunk_buffer = userdata;
+        size_t product, z;
+
+        product = size * nmemb;
+
+        z = realloc_buffer_size(chunk_buffer) + product;
+        if (z < realloc_buffer_size(chunk_buffer)) {
+                log_error("Overflow");
+                return 0;
+        }
+
+        if (z > (CA_PROTOCOL_SIZE_MAX - offsetof(CaProtocolChunk, data))) {
+                log_error("Chunk too large");
+                return 0;
+        }
+
+        if (!realloc_buffer_append(chunk_buffer, buffer, product)) {
+                log_oom();
+                return 0;
+        }
+
+        return product;
+}
+
+static void ca_chunk_downloader_free(CaChunkDownloader *dl) {
+        CURL *handle;
+        ChunkData *cd;
+
+        if (dl == NULL)
+                return;
+
+        while ((handle = queue_pop(dl->inprogress))) {
+                CURLMcode c;
+
+                c = curl_multi_remove_handle(dl->multi, handle);
+                if (c != CURLM_OK)
+                        log_error_curlm(c, "Failed to remove handle");
+
+                cd = get_curl_private(handle);
+                if (cd)
+                        chunk_data_free(cd);
+
+                curl_easy_cleanup(handle);
+        }
+
+        while ((handle = queue_pop(dl->ready))) {
+                cd = get_curl_private(handle);
+                if (cd)
+                        chunk_data_free(cd);
+
+                curl_easy_cleanup(handle);
+        }
+
+        while ((handle = queue_pop(dl->completed))) {
+                cd = get_curl_private(handle);
+                if (cd)
+                        chunk_data_free(cd);
+
+                curl_easy_cleanup(handle);
+        }
+
+        free(dl->store_url);
+        queue_free(dl->ready);
+        queue_free(dl->inprogress);
+        queue_free(dl->completed);
+        curl_multi_cleanup(dl->multi);
+        ca_remote_unref(dl->remote);
+
+        free(dl);
+}
+
+static inline void ca_chunk_downloader_freep(CaChunkDownloader **dl) {
+        ca_chunk_downloader_free(*dl);
+}
+
+static CaChunkDownloader *ca_chunk_downloader_new(CaRemote *rr, const char *store_url) {
+        CaChunkDownloader *dl = NULL;
+        uint64_t i;
+        int r;
+
+        dl = new0(CaChunkDownloader, 1);
+        if (!dl)
+                goto fail;
+
+        dl->remote = ca_remote_ref(rr);
+
+        r = make_curl_multi_handle(&dl->multi);
+        if (r < 0)
+                goto fail;
+
+        dl->ready = queue_new();
+        if (!dl->ready)
+                goto fail;
+
+        dl->inprogress = queue_new();
+        if (!dl->inprogress)
+                goto fail;
+
+        dl->completed = queue_new();
+        if (!dl->completed)
+                goto fail;
+
+        for (i = 0; i < arg_max_active_chunks; i++) {
+                CURL *handle = NULL;
+                ChunkData *cd = NULL;
+
+                cd = chunk_data_new();
+                if (!cd)
+                        goto fail;
+
+                r = make_curl_easy_handle(&handle, write_chunk, &cd->buffer, cd);
+                if (r < 0)
+                        goto fail;
+
+                queue_push(dl->ready, handle);
+        }
+
+        dl->store_url = strdup(store_url);
+        if (!dl->store_url)
+                goto fail;
+
+        return dl;
+
+fail:
+        ca_chunk_downloader_free(dl);
+        return NULL;
+}
+
+#define AVERAGE(sum, n) ({ typeof(sum) _sum = (sum); typeof(n) _n = (n); _n > 0 ? _sum / _n : _sum; })
+
+static void ca_chunk_downloader_display_stats(CaChunkDownloader *dl) {
+        if (!arg_verbose)
+                return;
+
+        log_info("--- Chunk Downloader Stats ---\n"
+                 "Iterations: %" PRIu64 "\n"
+                 "CURL handles totals: added=%" PRIu64 ", removed=%" PRIu64 "\n"
+                 "Chunks: %" PRIu64 " put to remote\n"
+                 "Queues average size: inprogress=%" PRIu64 ", completed=%" PRIu64 "\n"
+                 "Queues current size: inprogress=%" PRIu64 ", completed=%" PRIu64,
+                 dl->n_iterations,
+                 dl->inprogress->n_added, dl->inprogress->n_removed,
+                 dl->completed->n_removed,
+                 AVERAGE(dl->sum_inprogress_len, dl->n_iterations), AVERAGE(dl->sum_completed_len, dl->n_iterations),
+                 dl->inprogress->len, dl->completed->len);
+}
+
+static int configure_handle_for_chunk(CURL *handle, const char *store_url, CaChunkID *id) {
+        int r;
+        ChunkData *cd = NULL;
+        _cleanup_free_ char *url_buffer = NULL;
+
+        cd = get_curl_private(handle);
+        if (!cd)
+                return -EIO;
+
+        chunk_data_reset(cd, id);
+
+        url_buffer = chunk_url(store_url, id);
+        if (!url_buffer)
+                return log_oom();
+
+        r = configure_curl_easy_handle(handle, url_buffer);
+        if (r < 0)
+                return r;
+
+        return 0;
+}
+
+/* Get chunk requests from remote, configure curl handles accordingly,
+ * add to curl multi, and return the number of chunk requests handled. */
+static int ca_chunk_downloader_fetch_chunk_requests(CaChunkDownloader *dl) {
+        int i;
+        int n_handles_avail;
+
+        n_handles_avail = queue_len(dl->ready);
+
+        for (i = 0; i < n_handles_avail; i++) {
+                int r, running_handles;
+                CURLMcode c;
+                CaChunkID id;
+                CURL *handle;
+
+                r = ca_remote_has_pending_requests(dl->remote);
+                if (r < 0)
+                        return log_error_errno(r, "Failed to query pending requests: %m");
+                if (r == 0)
+                        break;
+
+                r = ca_remote_next_request(dl->remote, &id);
+                /* Even though we just ensured that there is a pending request,
+                 * it's possible that next_requests() returns -ENODATA */
+                if (r == -ENODATA)
+                        return 0;
+                if (r == -EPIPE)
+                        return r;
+                if (r < 0)
+                        return log_error_errno(r, "Failed to query next request: %m");
+
+                handle = queue_pop(dl->ready);
+                assert(handle);
+
+                r = configure_handle_for_chunk(handle, dl->store_url, &id);
+                if (r < 0)
+                        return log_error_errno(r, "Failed to configure handle: %m");
+
+                log_debug("Acquiring chunk %" PRIu64 ": %s", dl->inprogress->n_added,
+                          get_curl_effective_url(handle));
+
+                c = curl_multi_add_handle(dl->multi, handle);
+                if (c != CURLM_OK) {
+                        log_error_curlm(c, "Failed to add to multi handle");
+                        return -EIO;
+                }
+
+                queue_push(dl->inprogress, handle);
+
+                /* We know there must be something to do, since we just added something. */
+                c = curl_multi_perform(dl->multi, &running_handles);
+                if (c != CURLM_OK) {
+                        log_error_curlm(c, "Failed to perform curl multi");
+                        return -EIO;
+                }
+        }
+
+        return i;
+}
+
+/* Do the communication with the remote, return a status code */
+static int ca_chunk_downloader_remote_step(CaChunkDownloader *dl) {
+        int i;
+
+        for (i = 0; ; i++) {
+                int r;
+
+                r = ca_remote_step(dl->remote);
+                if (r == -EPIPE)
+                        return r;
+                if (r < 0)
+                        return log_error_errno(r, "Failed to process remoting engine: %m");
+
+                switch (r) {
+                case CA_REMOTE_POLL:
+                        r = CA_CHUNK_DOWNLOADER_POLL;
+                        return r;
+                case CA_REMOTE_FINISHED:
+                        r = CA_CHUNK_DOWNLOADER_FINISHED;
+                        return r;
+                case CA_REMOTE_STEP:
+                case CA_REMOTE_REQUEST:
+                        continue;
+                default:
+                        assert_not_reached("Unexpected step returned by remote_step()");
+                        break;
+                }
+        }
+
+        assert_not_reached("Should have returned");
+}
+
+/* Put chunk requests to the remote, return the number of chunks put */
+static int ca_chunk_downloader_put_chunks(CaChunkDownloader *dl) {
+        int i;
+
+        for (i = 0; ; i++) {
+                int r;
+                CURL *handle;
+                ChunkData *cd = NULL;
+
+                if (queue_is_empty(dl->completed))
+                        break;
+
+                r = ca_remote_can_put_chunk(dl->remote);
+                if (r == 0)
+                        break;
+                if (r == -EPIPE)
+                        return r;
+                if (r < 0)
+                        return log_error_errno(r, "Failed to query can put chunk: %m");
+
+                handle = queue_pop(dl->completed);
+                assert(handle);
+
+                cd = get_curl_private(handle);
+                if (!cd)
+                        return -EIO;
+
+                if (cd->current_store == SIZE_MAX) {
+                        r = ca_remote_put_missing(dl->remote, &cd->id);
+                        if (r < 0)
+                                return log_error_errno(r, "Failed to write missing message: %m");
+                } else {
+                        r = ca_remote_put_chunk(dl->remote, &cd->id, CA_CHUNK_COMPRESSED,
+                                                realloc_buffer_data(&cd->buffer),
+                                                realloc_buffer_size(&cd->buffer));
+                        if (r < 0)
+                                return log_error_errno(r, "Failed to write chunk: %m");
+                }
+
+                /* At this point, handle and chunk data are left "unconfigured"
+                 * in the ready queue. They'll be reconfigured when re-used. */
+                queue_push(dl->ready, handle);
+        }
+
+        return i;
+}
+
+/* Process chunks that were downloaded by curl, return the number of chunks handled */
+static int ca_chunk_downloader_process_curl_multi(CaChunkDownloader *dl) {
+        int i, n;
+        CURLMcode cm;
+
+        cm = curl_multi_perform(dl->multi, &n);
+        if (cm != CURLM_OK) {
+                log_error_curlm(cm, "Failed to perform curl multi");
+                return -EIO;
+        }
+
+        for (i = 0; ; i++) {
+                CURLcode c;
+                CURLMsg *msg;
+                CURL *handle;
+                long protocol_status;
+                const char *effective_url;
+                ChunkData *cd;
+
+                msg = curl_multi_info_read(dl->multi, &n);
+                if (!msg)
+                        break;
+
+                if (msg->msg != CURLMSG_DONE) {
+                        log_error("Unexpected CURL message: %d", msg->msg);
+                        return -EIO;
+                }
+
+                if (msg->data.result != CURLE_OK) {
+                        log_error_curle(msg->data.result, "Failed to acquire chunk");
+                        return -EIO;
+                }
+
+                handle = msg->easy_handle;
+
+                effective_url = get_curl_effective_url(handle);
+                if (!effective_url)
+                        return -EIO;
+
+                cd = get_curl_private(handle);
+                if (!cd)
+                        return -EIO;
+
+                c = curl_easy_getinfo(handle, CURLINFO_RESPONSE_CODE, &protocol_status);
+                if (c != CURLE_OK) {
+                        log_error_curle(c, "Failed to query response code");
+                        return -EIO;
+                }
+
+                if (!protocol_status_ok(arg_protocol, protocol_status)) {
+                        log_error("%s server failure %ld while requesting %s",
+                                  protocol_str(arg_protocol), protocol_status,
+                                  effective_url);
+
+                        // TODO support multiple stores
+
+                        /* No more stores? Set current_store to a special value
+                         * to indicate failure. */
+                        cd->current_store = SIZE_MAX;
+                }
+
+                cm = curl_multi_remove_handle(dl->multi, handle);
+                if (cm != CURLM_OK) {
+                        log_error_curlm(cm, "Failed to remove curl handle");
+                        return -EIO;
+                }
 
-static enum {
-        ARG_PROTOCOL_HTTP,
-        ARG_PROTOCOL_FTP,
-        ARG_PROTOCOL_HTTPS,
-        ARG_PROTOCOL_SFTP,
-        _ARG_PROTOCOL_INVALID = -1,
-} arg_protocol = _ARG_PROTOCOL_INVALID;
+                queue_remove(dl->inprogress, handle);
+                queue_push(dl->completed, handle);
+        }
 
-typedef enum ProcessUntil {
-        PROCESS_UNTIL_WRITTEN,
-        PROCESS_UNTIL_CAN_PUT_CHUNK,
-        PROCESS_UNTIL_CAN_PUT_INDEX,
-        PROCESS_UNTIL_CAN_PUT_ARCHIVE,
-        PROCESS_UNTIL_HAVE_REQUEST,
-        PROCESS_UNTIL_FINISHED,
-} ProcessUntil;
+        return i;
+}
 
-static CURLcode robust_curl_easy_perform(CURL *curl) {
-        uint64_t sleep_base_usec = 100 * 1000;
-        unsigned trial = 1;
-        unsigned limit = 10;
-        CURLcode c;
+static int ca_chunk_downloader_step(CaChunkDownloader *dl) {
+        int r;
 
-        assert(curl);
+        /* Handle curl activity */
+        r = ca_chunk_downloader_process_curl_multi(dl);
+        if (r < 0) {
+                log_error("Failed while processing curl multi");
+                return r;
+        }
+        if (r > 0)
+                log_trace("Processed %d curl messages", r);
 
-        while (trial < limit) {
+        /* Step around */
+        r = ca_chunk_downloader_remote_step(dl);
+        if (r == -EPIPE)
+                return r;
+        if (r < 0) {
+                log_error("Failed while processing remote engine");
+                return r;
+        }
+        if (r != CA_CHUNK_DOWNLOADER_POLL)
+                return r;
 
-                c = curl_easy_perform(curl);
+        /* Put as many downloaded chunks as we can */
+        r = ca_chunk_downloader_put_chunks(dl);
+        if (r == -EPIPE)
+                return r;
+        if (r < 0) {
+                log_error("Failed while putting chunks to remote");
+                return r;
+        }
+        if (r > 0)
+                log_trace("Put %d chunks to remote", r);
 
-                switch (c) {
+        /* Get as many chunk requests as we can */
+        r = ca_chunk_downloader_fetch_chunk_requests(dl);
+        if (r == -EPIPE)
+                return r;
+        if (r < 0) {
+                log_error("Failed while querying remote for chunk requests");
+                return r;
+        }
+        if (r > 0)
+                log_trace("Fetched %d chunk requests from remote", r);
 
-                case CURLE_COULDNT_CONNECT: {
-                        uint64_t sleep_usec;
+        return CA_CHUNK_DOWNLOADER_POLL;
+}
+
+static int get_remote_io_as_curl_waitfds(CaRemote *rr, struct curl_waitfd *ret_input,
+                                         struct curl_waitfd *ret_output) {
+        int r;
+        int input_fd, output_fd;
+        short input_poll_events, output_poll_events;
+        short input_curl_events, output_curl_events;
+
+        assert(rr);
+        assert(ret_input);
+        assert(ret_output);
+
+        r = ca_remote_get_io_fds(rr, &input_fd, &output_fd);
+        if (r < 0)
+                return r;
+
+        r = ca_remote_get_io_events(rr, &input_poll_events, &output_poll_events);
+        if (r < 0)
+                return r;
+
+        input_curl_events = input_poll_events & POLLIN ? CURL_WAIT_POLLIN : 0;
+        output_curl_events = output_poll_events & POLLOUT ? CURL_WAIT_POLLOUT : 0;
+
+        *ret_input = (struct curl_waitfd) {
+                .fd = input_fd,
+                .events = input_curl_events,
+        };
+
+        *ret_output = (struct curl_waitfd) {
+                .fd = output_fd,
+                .events = output_curl_events,
+
+        };
+
+        return 0;
+}
+
+static int ca_chunk_downloader_wait(CaChunkDownloader *dl) {
+        int n, r;
+        CURLMcode c;
+        int curl_timeout_ms = INT_MAX;
+        struct curl_waitfd waitfds[2] = {};
+
+        r = get_remote_io_as_curl_waitfds(dl->remote, &waitfds[0], &waitfds[1]);
+        if (r < 0)
+                return log_error_errno(r, "Failed to get remote io: %m");
+
+        log_trace("SLEEP  - handles: added=%" PRIu64 ", rem=%" PRIu64 " - chunks: put=%" PRIu64,
+                  dl->inprogress->n_added, dl->inprogress->n_removed, dl->completed->n_removed);
 
-                        /* Although this is not considered as a transient error by curl,
-                         * this error can happen momentarily while casync is retrieving
-                         * all the chunks from a remote. In this case we want to give
-                         * a break to the server and retry later.
-                         */
+        c = curl_multi_wait(dl->multi, waitfds, ELEMENTSOF(waitfds), curl_timeout_ms, &n);
+        if (c != CURLM_OK) {
+                log_error_curlm(c, "Failed to wait with curl multi");
+                return -EIO;
+        }
+
+        log_trace("AWAKEN - %d event(s)", n);
+
+        return 0;
+}
+
+static int download_chunks(CaChunkDownloader *dl) {
+        int r;
 
-                        sleep_usec = sleep_base_usec * trial;
-                        log_info("Could not connect, retrying in %" PRIu64 " ms", sleep_usec / 1000);
-                        usleep(sleep_usec);
-                        trial++;
+        for (;;) {
+                if (quit) {
+                        log_info("Got exit signal, quitting");
+                        r = 0;
                         break;
                 }
 
-                default:
-                        return c;
+                r = ca_chunk_downloader_step(dl);
+                if (r < 0)
+                        break;
+                if (r == CA_CHUNK_DOWNLOADER_FINISHED) {
+                        assert(queue_len(dl->inprogress) == 0);
+                        assert(queue_len(dl->completed) == 0);
+                        r = 0;
                         break;
                 }
+
+                r = ca_chunk_downloader_wait(dl);
+                if (r < 0)
+                        break;
+
+                dl->n_iterations++;
+                dl->sum_inprogress_len += dl->inprogress->len;
+                dl->sum_completed_len += dl->completed->len;
         }
 
-        return c;
+        return r;
 }
 
+/*
+ * archive/index download
+ */
+
 static int process_remote(CaRemote *rr, ProcessUntil until) {
         int r;
 
@@ -135,7 +1106,7 @@ static int process_remote(CaRemote *rr, ProcessUntil until) {
                                 return r;
                         if (r < 0)
                                 return log_error_errno(r, "Failed to determine whether there's more data to write.");
-                        if (r > 0)
+                        if (r == 0)
                                 return 0;
 
                         break;
@@ -180,7 +1151,7 @@ static size_t write_index(const void *buffer, size_t size, size_t nmemb, void *u
 
         r = ca_remote_put_index(rr, buffer, product);
         if (r < 0) {
-                log_error("Failed to put index: %m");
+                log_error_errno(r, "Failed to put index: %m");
                 return 0;
         }
 
@@ -200,6 +1171,10 @@ static int write_index_eof(CaRemote *rr) {
         if (r < 0)
                 return log_error_errno(r, "Failed to put index EOF: %m");
 
+        r = process_remote(rr, PROCESS_UNTIL_WRITTEN);
+        if (r < 0)
+                return r;
+
         return 0;
 }
 
@@ -216,10 +1191,14 @@ static size_t write_archive(const void *buffer, size_t size, size_t nmemb, void
 
         r = ca_remote_put_archive(rr, buffer, product);
         if (r < 0) {
-                log_error("Failed to put archive: %m");
+                log_error_errno(r, "Failed to put archive: %m");
                 return 0;
         }
 
+        r = process_remote(rr, PROCESS_UNTIL_WRITTEN);
+        if (r < 0)
+                return r;
+
         return product;
 }
 
@@ -239,130 +1218,44 @@ static int write_archive_eof(CaRemote *rr) {
         return 0;
 }
 
-static size_t write_chunk(const void *buffer, size_t size, size_t nmemb, void *userdata) {
-        ReallocBuffer *chunk_buffer = userdata;
-        size_t product, z;
-
-        product = size * nmemb;
-
-        z = realloc_buffer_size(chunk_buffer) + product;
-        if (z < realloc_buffer_size(chunk_buffer)) {
-                log_error("Overflow");
-                return 0;
-        }
-
-        if (z > (CA_PROTOCOL_SIZE_MAX - offsetof(CaProtocolChunk, data))) {
-                log_error("Chunk too large");
-                return 0;
-        }
-
-        if (!realloc_buffer_append(chunk_buffer, buffer, product)) {
-                log_oom();
-                return 0;
-        }
-
-        return product;
-}
-
-static char *chunk_url(const char *store_url, const CaChunkID *id) {
-        char ids[CA_CHUNK_ID_FORMAT_MAX], *buffer;
-        const char *suffix;
-        size_t n;
-
-        /* Chop off URL arguments and multiple trailing dashes, then append the chunk ID and ".cacnk" */
-
-        suffix = ca_compressed_chunk_suffix();
-
-        n = strcspn(store_url, "?;");
-        while (n > 0 && store_url[n-1] == '/')
-                n--;
-
-        buffer = new(char, n + 1 + 4 + 1 + CA_CHUNK_ID_FORMAT_MAX-1 + strlen(suffix) + 1);
-        if (!buffer)
-                return NULL;
-
-        ca_chunk_id_format(id, ids);
-
-        strcpy(mempcpy(mempcpy(mempcpy(mempcpy(mempcpy(buffer, store_url, n), "/", 1), ids, 4), "/", 1), ids, CA_CHUNK_ID_FORMAT_MAX-1), suffix);
-
-        return buffer;
-}
-
-static int acquire_file(CaRemote *rr,
-                        CURL *curl,
-                        const char *url,
-                        size_t (*callback)(const void *p, size_t size, size_t nmemb, void *userdata)) {
-
+static int acquire_file(CaRemote *rr, CURL *handle) {
+        CURLcode c;
         long protocol_status;
 
-        assert(curl);
-        assert(url);
-        assert(callback);
-
-        if (curl_easy_setopt(curl, CURLOPT_URL, url) != CURLE_OK) {
-                log_error("Failed to set CURL URL to: %s", url);
-                return -EIO;
-        }
-
-        if (curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, callback) != CURLE_OK) {
-                log_error("Failed to set CURL callback function.");
-                return -EIO;
-        }
-
-        if (curl_easy_setopt(curl, CURLOPT_WRITEDATA, rr) != CURLE_OK) {
-                log_error("Failed to set CURL private data.");
-                return -EIO;
-        }
-
-        log_debug("Acquiring %s...", url);
+        log_debug("Acquiring %s...", get_curl_effective_url(handle));
 
-        if (robust_curl_easy_perform(curl) != CURLE_OK) {
-                log_error("Failed to acquire %s", url);
+        c = curl_easy_perform(handle);
+        if (c != CURLE_OK) {
+                log_error("Failed to acquire %s", get_curl_effective_url(handle));
                 return -EIO;
         }
 
-        if (curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &protocol_status) != CURLE_OK) {
+        c = curl_easy_getinfo(handle, CURLINFO_RESPONSE_CODE, &protocol_status);
+        if (c != CURLE_OK) {
                 log_error("Failed to query response code");
                 return -EIO;
         }
 
-        if (IN_SET(arg_protocol, ARG_PROTOCOL_HTTP, ARG_PROTOCOL_HTTPS) && protocol_status != 200) {
-                char *m;
-
-                if (arg_verbose)
-                        log_error("HTTP server failure %li while requesting %s.", protocol_status, url);
-
-                if (asprintf(&m, "HTTP request on %s failed with status %li", url, protocol_status) < 0)
-                        return log_oom();
-
-                (void) ca_remote_abort(rr, protocol_status == 404 ? ENOMEDIUM : EBADR, m);
-                free(m);
-
-                return 0;
-
-        } else if (arg_protocol == ARG_PROTOCOL_FTP && (protocol_status < 200 || protocol_status > 299)) {
-                char *m;
+        if (!protocol_status_ok(arg_protocol, protocol_status)) {
+                _cleanup_free_ char *message = NULL;
+                int abort_code;
 
                 if (arg_verbose)
-                        log_error("FTP server failure %li while requesting %s.", protocol_status, url);
+                        log_error("%s server failure %li while requesting %s",
+                                  protocol_str(arg_protocol), protocol_status,
+                                  get_curl_effective_url(handle));
 
-                if (asprintf(&m, "FTP request on %s failed with status %li", url, protocol_status) < 0)
+                if (asprintf(&message, "%s request on %s failed with status %li",
+                             protocol_str(arg_protocol), get_curl_effective_url(handle),
+                             protocol_status) < 0)
                         return log_oom();
 
-                (void) ca_remote_abort(rr, EBADR, m);
-                free(m);
-                return 0;
-        } else if (arg_protocol == ARG_PROTOCOL_SFTP && (protocol_status != 0)) {
-                char *m;
-
-                if (arg_verbose)
-                        log_error("SFTP server failure %li while requesting %s.", protocol_status, url);
-
-                if (asprintf(&m, "SFTP request on %s failed with status %li", url, protocol_status) < 0)
-                        return log_oom();
+                if (IN_SET(arg_protocol, PROTOCOL_HTTP, PROTOCOL_HTTPS) && protocol_status == 404)
+                        abort_code = ENOMEDIUM;
+                else
+                        abort_code = EBADR;
 
-                (void) ca_remote_abort(rr, EBADR, m);
-                free(m);
+                (void) ca_remote_abort(rr, abort_code, message);
                 return 0;
         }
 
@@ -372,11 +1265,7 @@ static int acquire_file(CaRemote *rr,
 static int run(int argc, char *argv[]) {
         const char *base_url, *archive_url, *index_url, *wstore_url;
         size_t n_stores = 0, current_store = 0;
-        CURL *curl = NULL;
         _cleanup_(ca_remote_unrefp) CaRemote *rr = NULL;
-        _cleanup_(realloc_buffer_free) ReallocBuffer chunk_buffer = {};
-        _cleanup_free_ char *url_buffer = NULL;
-        long protocol_status;
         int r;
 
         if (argc < _CA_REMOTE_ARG_MAX) {
@@ -414,60 +1303,28 @@ static int run(int argc, char *argv[]) {
                                               (index_url ? CA_PROTOCOL_READABLE_INDEX : 0) |
                                               (archive_url ? CA_PROTOCOL_READABLE_ARCHIVE : 0));
         if (r < 0) {
-                log_error("Failed to set feature flags: %m");
+                log_error_errno(r, "Failed to set feature flags: %m");
                 goto finish;
         }
 
         r = ca_remote_set_io_fds(rr, STDIN_FILENO, STDOUT_FILENO);
         if (r < 0) {
-                log_error("Failed to set I/O file descriptors: %m");
-                goto finish;
-        }
-
-        curl = curl_easy_init();
-        if (!curl) {
-                r = log_oom();
-                goto finish;
-        }
-
-        if (curl_easy_setopt(curl, CURLOPT_FOLLOWLOCATION, 1L) != CURLE_OK) {
-                log_error("Failed to turn on location following.");
-                r = -EIO;
-                goto finish;
-        }
-
-        if (curl_easy_setopt(curl, CURLOPT_PROTOCOLS, arg_protocol == ARG_PROTOCOL_FTP ? CURLPROTO_FTP :
-                                                      arg_protocol == ARG_PROTOCOL_SFTP? CURLPROTO_SFTP: CURLPROTO_HTTP|CURLPROTO_HTTPS) != CURLE_OK) {
-                log_error("Failed to limit protocols to HTTP/HTTPS/FTP/SFTP.");
-                r = -EIO;
+                log_error_errno(r, "Failed to set I/O file descriptors: %m");
                 goto finish;
         }
 
-        if (arg_protocol == ARG_PROTOCOL_SFTP) {
-                /* activate the ssh agent. For this to work you need
-                   to have ssh-agent running (type set | grep SSH_AGENT to check) */
-                if (curl_easy_setopt(curl, CURLOPT_SSH_AUTH_TYPES, CURLSSH_AUTH_AGENT) != CURLE_OK)
-                        log_error("Failed to turn on ssh agent support, ignoring.");
-        }
+        if (archive_url) {
+                _cleanup_(curl_easy_cleanupp) CURL *handle = NULL;
 
-        if (arg_rate_limit_bps > 0) {
-                if (curl_easy_setopt(curl, CURLOPT_MAX_SEND_SPEED_LARGE, arg_rate_limit_bps) != CURLE_OK) {
-                        log_error("Failed to set CURL send speed limit.");
-                        r = -EIO;
+                r = make_curl_easy_handle(&handle, write_archive, rr, NULL);
+                if (r < 0)
                         goto finish;
-                }
 
-                if (curl_easy_setopt(curl, CURLOPT_MAX_RECV_SPEED_LARGE, arg_rate_limit_bps) != CURLE_OK) {
-                        log_error("Failed to set CURL receive speed limit.");
-                        r = -EIO;
+                r = configure_curl_easy_handle(handle, archive_url);
+                if (r < 0)
                         goto finish;
-                }
-        }
 
-        /* (void) curl_easy_setopt(curl, CURLOPT_VERBOSE, 1L); */
-
-        if (archive_url) {
-                r = acquire_file(rr, curl, archive_url, write_archive);
+                r = acquire_file(rr, handle);
                 if (r < 0)
                         goto finish;
                 if (r == 0)
@@ -479,138 +1336,51 @@ static int run(int argc, char *argv[]) {
         }
 
         if (index_url) {
-                r = acquire_file(rr, curl, index_url, write_index);
+                _cleanup_(curl_easy_cleanupp) CURL *handle = NULL;
+
+                r = make_curl_easy_handle(&handle, write_index, rr, NULL);
                 if (r < 0)
                         goto finish;
-                if (r == 0)
-                        goto flush;
 
-                r = write_index_eof(rr);
+                r = configure_curl_easy_handle(handle, index_url);
                 if (r < 0)
                         goto finish;
-        }
-
-        for (;;) {
-                const char *store_url;
-                CaChunkID id;
 
-                if (quit) {
-                        log_info("Got exit signal, quitting.");
-                        r = 0;
+                r = acquire_file(rr, handle);
+                if (r < 0)
                         goto finish;
-                }
-
-                if (n_stores == 0)  /* No stores? Then we did all we could do */
-                        break;
+                if (r == 0)
+                        goto flush;
 
-                r = process_remote(rr, PROCESS_UNTIL_HAVE_REQUEST);
-                if (r == -EPIPE) {
-                        r = 0;
-                        goto finish;
-                }
+                r = write_index_eof(rr);
                 if (r < 0)
                         goto finish;
+        }
 
-                r = ca_remote_next_request(rr, &id);
-                if (r == -ENODATA)
-                        continue;
-                if (r < 0) {
-                        log_error_errno(r, "Failed to determine next chunk to get: %m");
-                        goto finish;
-                }
+        if (n_stores > 0) {
+                _cleanup_(ca_chunk_downloader_freep) CaChunkDownloader *dl = NULL;
+                const char *store_url;
 
+                // TODO support multiple stores
+                /* set the store url */
                 current_store = current_store % n_stores;
                 if (wstore_url)
-                        store_url = current_store == 0 ? wstore_url : argv[current_store + _CA_REMOTE_ARG_MAX - 1];
+                        store_url = current_store == 0 ? wstore_url :
+                                argv[current_store + _CA_REMOTE_ARG_MAX - 1];
                 else
                         store_url = argv[current_store + _CA_REMOTE_ARG_MAX];
                 /* current_store++; */
 
-                free(url_buffer);
-                url_buffer = chunk_url(store_url, &id);
-                if (!url_buffer) {
+                dl = ca_chunk_downloader_new(rr, store_url);
+                if (!dl) {
                         r = log_oom();
                         goto finish;
                 }
 
-                if (curl_easy_setopt(curl, CURLOPT_URL, url_buffer) != CURLE_OK) {
-                        log_error("Failed to set CURL URL to: %s", index_url);
-                        r = -EIO;
-                        goto finish;
-                }
-
-                if (curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, write_chunk) != CURLE_OK) {
-                        log_error("Failed to set CURL callback function.");
-                        r = -EIO;
-                        goto finish;
-                }
-
-                if (curl_easy_setopt(curl, CURLOPT_WRITEDATA, &chunk_buffer) != CURLE_OK) {
-                        log_error("Failed to set CURL private data.");
-                        r = -EIO;
-                        goto finish;
-                }
-
-                if (arg_rate_limit_bps > 0) {
-                        if (curl_easy_setopt(curl, CURLOPT_MAX_SEND_SPEED_LARGE, arg_rate_limit_bps) != CURLE_OK) {
-                                log_error("Failed to set CURL send speed limit.");
-                                r = -EIO;
-                                goto finish;
-                        }
-
-                        if (curl_easy_setopt(curl, CURLOPT_MAX_RECV_SPEED_LARGE, arg_rate_limit_bps) != CURLE_OK) {
-                                log_error("Failed to set CURL receive speed limit.");
-                                r = -EIO;
-                                goto finish;
-                        }
-                }
-
-                log_debug("Acquiring %s...", url_buffer);
-
-                if (robust_curl_easy_perform(curl) != CURLE_OK) {
-                        log_error("Failed to acquire %s", url_buffer);
-                        r = -EIO;
-                        goto finish;
-                }
-
-                if (curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &protocol_status) != CURLE_OK) {
-                        log_error("Failed to query response code");
-                        r = -EIO;
-                        goto finish;
-                }
-
-                r = process_remote(rr, PROCESS_UNTIL_CAN_PUT_CHUNK);
-                if (r == -EPIPE) {
-                        r = 0;
-                        goto finish;
-                }
-                if (r < 0)
-                        goto finish;
-
-                if ((IN_SET(arg_protocol, ARG_PROTOCOL_HTTP, ARG_PROTOCOL_HTTPS) && protocol_status == 200) ||
-                    (arg_protocol == ARG_PROTOCOL_FTP && (protocol_status >= 200 && protocol_status <= 299))||
-                    (arg_protocol == ARG_PROTOCOL_SFTP && (protocol_status == 0))) {
+                r = download_chunks(dl);
 
-                        r = ca_remote_put_chunk(rr, &id, CA_CHUNK_COMPRESSED, realloc_buffer_data(&chunk_buffer), realloc_buffer_size(&chunk_buffer));
-                        if (r < 0) {
-                                log_error_errno(r, "Failed to write chunk: %m");
-                                goto finish;
-                        }
-
-                } else {
-                        if (arg_verbose)
-                                log_error("HTTP/FTP/SFTP server failure %li while requesting %s.", protocol_status, url_buffer);
-
-                        r = ca_remote_put_missing(rr, &id);
-                        if (r < 0) {
-                                log_error_errno(r, "Failed to write missing message: %m");
-                                goto finish;
-                        }
-                }
+                ca_chunk_downloader_display_stats(dl);
 
-                realloc_buffer_empty(&chunk_buffer);
-
-                r = process_remote(rr, PROCESS_UNTIL_WRITTEN);
                 if (r == -EPIPE) {
                         r = 0;
                         goto finish;
@@ -623,9 +1393,6 @@ flush:
         r = process_remote(rr, PROCESS_UNTIL_FINISHED);
 
 finish:
-        if (curl)
-                curl_easy_cleanup(curl);
-
         return r;
 }
 
@@ -635,26 +1402,33 @@ static void help(void) {
 
 static int parse_argv(int argc, char *argv[]) {
 
+        enum {
+              ARG_MAX_ACTIVE_CHUNKS = 0x100,
+              ARG_MAX_HOST_CONNECTIONS,
+        };
+
         static const struct option options[] = {
-                { "help",           no_argument,       NULL, 'h'                },
-                { "verbose",        no_argument,       NULL, 'v'                },
-                { "rate-limit-bps", required_argument, NULL, 'l'                },
+                { "help",                 no_argument,       NULL, 'h'                      },
+                { "verbose",              no_argument,       NULL, 'v'                      },
+                { "rate-limit-bps",       required_argument, NULL, 'l'                      },
+                { "max-active-chunks",    required_argument, NULL, ARG_MAX_ACTIVE_CHUNKS    },
+                { "max-host-connections", required_argument, NULL, ARG_MAX_HOST_CONNECTIONS },
                 {}
         };
 
-        int c;
+        int c, r;
 
         assert(argc >= 0);
         assert(argv);
 
         if (strstr(argv[0], "https"))
-                arg_protocol = ARG_PROTOCOL_HTTPS;
+                arg_protocol = PROTOCOL_HTTPS;
         else if (strstr(argv[0], "http"))
-                arg_protocol = ARG_PROTOCOL_HTTP;
+                arg_protocol = PROTOCOL_HTTP;
         else if (strstr(argv[0], "sftp"))
-                arg_protocol = ARG_PROTOCOL_SFTP;
+                arg_protocol = PROTOCOL_SFTP;
         else if (strstr(argv[0], "ftp"))
-                arg_protocol = ARG_PROTOCOL_FTP;
+                arg_protocol = PROTOCOL_FTP;
         else {
                 log_error("Failed to determine set of protocols to use, refusing.");
                 return -EINVAL;
@@ -663,7 +1437,7 @@ static int parse_argv(int argc, char *argv[]) {
         if (getenv_bool("CASYNC_VERBOSE") > 0)
                 arg_verbose = true;
 
-        while ((c = getopt_long(argc, argv, "hv", options, NULL)) >= 0) {
+        while ((c = getopt_long(argc, argv, "hvl", options, NULL)) >= 0) {
 
                 switch (c) {
 
@@ -679,6 +1453,22 @@ static int parse_argv(int argc, char *argv[]) {
                         arg_rate_limit_bps = strtoll(optarg, NULL, 10);
                         break;
 
+                case ARG_MAX_ACTIVE_CHUNKS:
+                        r = safe_atou64(optarg, &arg_max_active_chunks);
+                        if (r < 0 || arg_max_active_chunks == 0) {
+                                log_error("Invalid value for max-active-chunks, refusing");
+                                return -EINVAL;
+                        }
+                        break;
+
+                case ARG_MAX_HOST_CONNECTIONS:
+                        r = safe_atou64(optarg, &arg_max_host_connections);
+                        if (r < 0 || arg_max_host_connections == 0) {
+                                log_error("Invalid value for max-host-connections, refusing");
+                                return -EINVAL;
+                        }
+                        break;
+
                 case '?':
                         return -EINVAL;
 
-- 
2.20.1

